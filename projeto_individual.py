# -*- coding: utf-8 -*-
"""Projeto_individual.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XAzapqBlcJlZjG0wWpPXtVyLgz1q9DsJ

#Instações das Bibliotecas que não são nativas do Colab
"""

pip install pymongo[srv]

pip install pandera

!pip install gcsfs

pip install pyspark

"""#Importando as bibliotecas"""

import pandas as pd
from google.cloud import storage
import os

import pandera as pa
import numpy as np

from pyspark.sql import SparkSession
from pyspark import SparkConf
import pyspark.sql.functions as F
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DataType

from pyspark.sql.window import Window
from pyspark.sql.functions import round

import pymongo
from pymongo import MongoClient

"""#Conexão com GCP

Usado from google.colab import drive
drive.mount('/content/drive') APENAS PARA IMPORTAR A KEY JSON!
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Criação das credenciais de comunicação com o gcp"""

serviceAccount = '/content/projeto-individual-344416-a1a791d46a36.json'

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = serviceAccount

"""CRIANDO UMA VARIÁVEL CHAMADA BUCKET PARA RECEBER O NOME DA BUCKET DO CLOUD STORAGE

USANDO MÉTODO BLOB PARA RETORNAR O NOME DO ARQUIVO 

CRIANDO A VARIÁVEL PATH PARA COLOCAR O CAMINHO DO ARQUIVO CSV
"""

client = storage.Client()

bucket = client.get_bucket('projeto_individual_jozi')
bucket.blob('marketing_campaign.csv - marketing_campaign.csv')
path = 'gs://projeto_individual_jozi/DataSet Original/marketing_campaign.csv - marketing_campaign.csv.csv'

"""NOMEANDO DATAFRAME: df"""

df = pd.read_csv(path, sep = ',')

"""# CONECTANDO MONGO"""

clients = pymongo.MongoClient('mongodb+srv://soulcode:a1b2c3@cluster0.tdxv5.mongodb.net/myFirstDatabase?retryWrites=true&w=majority')

db = clients['projeto_individual']
colecao = db['original']
df.reset_index(inplace= False)
df_dici = df.to_dict("records")
colecao.insert_many(df_dici)

serviceAccount = '/content/projeto-individual-344416-a1a791d46a36.json'

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = serviceAccount

cliente1 = storage.Client()

bucket = client.get_bucket('projeto_individual_jozi')

bucket.blob('DataFrameTratado/marketing_campaign-tratado.csv').upload_from_string(df_tratado.to_csv(index = False), 'csv')

"""#PANDAS"""

pd.set_option('max_columns', None) #COMANDO PARA VISUALIZAR TODAS AS COLUNAS

"""#LENDO DF"""

df

df.shape

df.dtypes

df.info()

df.head(2)

"""#TRADUZINDO DF"""

df.rename(columns={'Year_Birth':'Ano_Nascimento', 'Education':'Escolaridade', 'Marital_Status':'Estado_Civil', 'Income':'Renda', 'Kidhome':'Criancas_Casa', ' Teenhome':'Adolescente_Casa', 'Dt_Customer':'Data_Cadastro', 'Recency':'Compra_Recente', 'MntWines':'Gasto_Vinho', 'MntFruits':'Gasto_Frutas', 'MntMeatProducts':'Gasto_Carne', 'MntFishProducts':'Gasto_Peixe', 'MntSweetProducts':'Gasto_Doces', 'MntGoldProds':'Gasto_Ouro', 'NumDealsPurchases':'Compras_Desconto', 'NumWebPurchases':'Compras_Web', 'NumCatalogPurchases':'Compras_Catalogo', 'NumStorePurchases':'Compras_loja', 'NumWebVisitsMonth':'Visitas_Web_mes', 'AcceptedCmp3':'Aceitou_Campanha3', 'AcceptedCmp4':'Aceitou_campanha4', 'AcceptedCmp5':'Aceitou_campanha5', 'AcceptedCmp1':'Aceitou_Campanha1', 'AcceptedCmp2':'Aceitou_Campanha2', 'Complain':'Reclamacao', 'Z_CostContact':'Custo_Contato', 'Z_Revenue':'Receita', 'Response':'Resposta'}, inplace = True)

pd.unique(df['Escolaridade'])

df['Escolaridade'].replace(['Graduation', 'Master', 'Basic', '2n Cycle'], ['Graduação', 'Mestrado', 'Ensino Básico', 'Pós Graduação'], inplace = True)

pd.unique(df['Estado_Civil'])

"""Obs: No dado "Alone", traduzindo significa Sozinho, mas na análise, interpretou - se que a pessoa é solteira porque quer, ou seja, solteiro convicto!

YOYO é uma expressão usada por americanos, no entanto interpretou - se como Curtindo a Vida!  

 Não há uma tradução específica para Absurd, pois o mesmo não apresenta um sentido correto de estado civil!
"""

df['Estado_Civil'].replace(['Single', 'Together', 'Married', 'Divorced', 'Widow', 'Alone','Absurd', 'YOLO'], ['Solteiro', 'União_Estável', 'Casado', 'Divorciado', 'Viuvo', 'Solteiro_Convicto', 'Absurd', 'Curtindo_a_Vida'], inplace = True)

df.head(2)

"""#Analisando os Dados

Dropando colunas que na análise não é preciso usar: Compra_Recente, Data_Cadastro, Reclamacao

Dropando colunas sem sentido: Custo_Contato, Receita, Resposta 

Renomeando algumas colunas
"""

df.ID.is_unique

df.rename(columns={'Teenhome':'Adolescente_Casa'},inplace = True)

df.rename(columns={'Criancas_Casa':'Criancas', 'Adolescente_Casa':'Adolescente'},inplace = True)

df.drop(['Compra_Recente','Data_Cadastro','Reclamacao','Custo_Contato', 'Receita', 'Resposta'], axis=1, inplace=True)

"""#Backup do DF"""

df2 = df.copy

"""#Limpeza de Dados

Valores nulos foram encontrados apenas na coluna Renda, porém os valores já foram tratados como nan, sendo assim não foi preciso realizar a limpeza dos mesmos.
"""

pd.unique(df['Renda'])

#Soma dos valores nulos encontrados
df.isna().sum()

"""#Filtro dos dados nulos 'nan'"""

filtronulos = df.Renda.isna() 
df.loc[filtronulos]

"""Analisando a renda media 

Foi adicionado uma variável e extraído a média
"""

renda_media = df.Renda.median()

renda_media

"""Preenchimento dos valores nulos da coluna Renda com os valores da média!"""

df.fillna(renda_media, inplace = True)

"""Verificação se realmente os dados nulos foram preenchidos"""

df.Renda.isnull().sum

filtronulos1 = df.Renda.isna() 
df.loc[filtronulos1]

df.head(5)

"""#PYSPARK

Optou-se por não importar o dataset original e sim usar o mesmo(dataset) já com o tratamento realizado no Pandas!
"""

spark = (
    SparkSession.builder
                .master('local')
                .appName('projetoIndividual')
                .config('spark.ui.port', '4050')
                .config("spark.jars", 'https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar')
                .getOrCreate()
)

"""# Montando a estrutura do DataFrame utilizando o StructType"""

df_spark = spark.createDataFrame(df, schema = schema)

schema = ( StructType ([
    StructField('ID',StringType(), True),
    StructField('Ano_Nascimento', StringType(), True),
    StructField('Escolaridade', StringType(), True),
    StructField('Estado_Civil', StringType(), False),
    StructField('Renda', FloatType(), True),
    StructField('Crianças', IntegerType(), True),
    StructField('Adolescentes', IntegerType(), True),
    StructField('Gasto_Vinho', IntegerType(), True),
    StructField('Gasto_Frutas',IntegerType(), True), 
    StructField('Gasto_Carne', IntegerType(), True),
    StructField('Gasto_Peixe', IntegerType(), True),
    StructField('Gasto_Doces', IntegerType(), True),
    StructField('Gasto_Ouro',IntegerType(), True),
    StructField('Compras_Desconto', IntegerType(), True),
    StructField('Compras_Web', IntegerType(), True),
    StructField('Compras_Catalogo', IntegerType(), True),
    StructField('Compras_loja', IntegerType(), True),
    StructField('Visitas_Web_mes', IntegerType(), True), 
    StructField('Aceitou_Campanha3',IntegerType(), True),
    StructField('Aceitou_campanha4',IntegerType(), True),
    StructField('Aceitou_campanha5',IntegerType(), True),
    StructField('Aceitou_Campanha1',IntegerType(), True),
    StructField('Aceitou_Campanha2',IntegerType(), True),
    ])
)

df_spark.show()

df_spark.printSchema()

"""#Backup do df_spark"""

df_spark1 = df_spark

"""#Análise dos dados inconsistentes, nulos

Não foi encontrado dados inconsistentes, pois os mesmos já estavam tratados como nan, mas eu usei fillna no pandas para trocar os dados nulos pela média
"""

for c in df_spark.columns:
  print(c, df_spark.filter(F.col(c).isNull()).count())

"""# Não foi preciso dropar colunas, pois já realizado no pandas

#Renomeando colunas

Usando comando withColumnRenamed
"""

df_spark.select(F.col('ID')).distinct().count()

df_spark = df_spark.withColumnRenamed('Renda', 'Renda_Anual')
df_spark = df_spark.withColumnRenamed('Visitas_Web_mes', 'Visitas_site_mes')
df_spark.printSchema()

"""# Criando Colunas

Criando coluna para Aceite de campanha
"""

df_spark1 = df_spark.withColumn('Aceitou_campanhas', F.col('Aceitou_Campanha3') + F.col('Aceitou_campanha4') + F.col('Aceitou_campanha5') + F.col('Aceitou_campanha1') + F.col('Aceitou_campanha2')) 
df_spark1.show(2)

"""Criando coluna para Gasto total"""

df_spark2 = df_spark.withColumn('GastoTotal', F.col('Gasto_Vinho') + F.col('Gasto_Frutas') + F.col('Gasto_Carne') + F.col('Gasto_Peixe') + F.col('Gasto_Doces') + F.col('Gasto_Ouro'))
df_spark2.show(2)

df_spark.show(2)

"""Filtros, ordenação e agrupamento

Media de gasto por itens de acordo com o estado civil
"""

df_spark3 = df_spark.groupBy("Estado_Civil").agg({'Gasto_Vinho':'mean', 'Gasto_Frutas': 'mean', 'Gasto_Carne':'mean', 'Gasto_Peixe': 'mean'}).show()

"""Quem tem criança gasta mais com doces e frutas?"""

df_spark4 = df_spark.groupBy('Crianças').agg({'Gasto_Frutas':'mean','Gasto_Doces':'mean' }).show()

"""Compra com desconto por causa das campanhas"""

df_spark5 = df_spark1.select(F.col('Compras_Desconto'), F.col('Aceitou_campanhas')).filter(F.col('Aceitou_campanhas') > 0).show(truncate = False)

df_spark6 = df_spark.groupBy('Escolaridade').count().orderBy('count', ascending=False).show()

df_spark6 = df_spark.groupBy('Estado_Civil').count().orderBy('count', ascending=False).show()

"""Window Functions"""

w0=Window.partitionBy(F.col('Escolaridade')).orderBy('ID')

w1 = Window.partitionBy(F.col('Ano_Nascimento')).orderBy('ID')

w2 = Window.partitionBy(F.col('Compras_Desconto')).orderBy('Aceitou_campanhas')

"""#Transformando em pandas novamente para exportar para a GCP"""

df_tratado = df_pandas = df_spark.toPandas()
df_tratado

"""# SparkSQL"""

spark = (
    SparkSession.builder
                .master('local')
                .appName('projeto')
                .config('spark.ui.port', '4050')
                .config("spark.csv", 'https://storage.googleapis.com/projeto_individual_jozi/DataSet%20Original/marketing_campaign.csv%20-%20marketing_campaign.csv.csv')
                .getOrCreate()
)

dfsql = spark.createDataFrame(df_tratado)
dfsql.show()

dfsql.SELECT('Escolaridade'), MEAN('Compras_Desconto').show()

dfsql.SELECT * FROM ('Estado_Civil') WHERE ('Gasto_Vinho').show

"""#Conectando df_tratado com GCP"""

clients = pymongo.MongoClient('mongodb+srv://soulcode:a1b2c3@cluster0.tdxv5.mongodb.net/myFirstDatabase?retryWrites=true&w=majority')

db = clients['projeto_individual']
colecao = db['dftratado']
df_tratado.reset_index(inplace= False)
df_dici = df.to_dict("records")
colecao.insert_many(df_dici)

extrator = colecao.find({})
dbjm = pd.DataFrame(list(extrator))

serviceAccount = '/content/projeto-individual-344416-a1a791d46a36.json'

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = serviceAccount

cliente = storage.Client()

bucket = client.get_bucket('projeto_individual_jozi')

bucket.blob('DataFrameTratado/marketing_campaign-tratado.csv').upload_from_string(df_tratado.to_csv(index = False), 'csv')